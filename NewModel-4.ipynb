{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccd4a0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asiak\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Asiak\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\model_selection\\_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\Asiak\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_scorer.py\", line 103, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"C:\\Users\\Asiak\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_scorer.py\", line 264, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"C:\\Users\\Asiak\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_regression.py\", line 521, in mean_squared_log_error\n",
      "    raise ValueError(\n",
      "ValueError: Mean Squared Logarithmic Error cannot be used when targets contain negative values.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor: 0.2340414516143391\n",
      "Decision Tree Regressor: 0.3627989425325001\n",
      "Gradient Boosting Regressor: nan\n",
      "Best model: Random Forest Regressor\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the data into a pandas dataframe\n",
    "\n",
    "Train_df = pd.read_csv('train.csv')\n",
    "stores_df = pd.read_csv('stores.csv')\n",
    "Transaction_df = pd.read_csv('transactions.csv')\n",
    "oil_df = pd.read_csv('oil.csv')\n",
    "submission_df = pd.read_csv('submission.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "Holiday_df = pd.read_csv('holidays_events.csv')\n",
    "\n",
    "\n",
    "merged_df = pd.merge(Train_df, stores_df, on='store_nbr', how='left')\n",
    "merged_df = pd.merge(merged_df,Transaction_df, on=['store_nbr','date'], how='left')\n",
    "merged_df = pd.merge(merged_df,oil_df, on=['date'], how='left')\n",
    "data = pd.merge(merged_df,Holiday_df, on=['date'], how='left')\n",
    "\n",
    "#rename column\n",
    "data.rename(columns = {'type_x':'Store_type'}, inplace = True)\n",
    "data.rename(columns = {'type_y':'Holiday_type'}, inplace = True)\n",
    "\n",
    "# Drop the id column since it's not relevant for modeling\n",
    "data = data.drop('id', axis=1)\n",
    "\n",
    "# Convert the date column to a datetime object\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Set the date column as the index\n",
    "data = data.set_index('date')\n",
    "\n",
    "#Creating the Day, Month and Year column from the Date Column\n",
    "data['year'] = data.index.year\n",
    "data['month'] =data.index.month\n",
    "data['day']=data.index.day\n",
    "#data['dayofweek']=data.index.weekday\n",
    "#data['end_month']=data.index.is_month_end\n",
    "\n",
    "#Implement the new super grouping of product family on the actual family attribute. \n",
    "\n",
    "data['family'] = data['family'].replace({\n",
    "'AUTOMOTIVE': 'Others',\n",
    "'BABY CARE': 'Personal Care',\n",
    "'BEAUTY': 'Personal Care',\n",
    "'BEVERAGES': 'Beverages',\n",
    "'BOOKS': 'Others',\n",
    "'BREAD/BAKERY': 'Food',\n",
    "'CELEBRATION': 'Food',\n",
    "'CLEANING': 'Others',\n",
    "'DAIRY': 'Food',\n",
    "'DELI': 'Food',\n",
    "'EGGS': 'Food',\n",
    "'FROZEN FOODS': 'Food',\n",
    "'GROCERY I': 'Food',\n",
    "'GROCERY II': 'Food',\n",
    "'HARDWARE': 'Others',\n",
    "'HOME AND KITCHEN I': 'Home and Kitchen',\n",
    "'HOME AND KITCHEN II': 'Home and Kitchen',\n",
    "'HOME APPLIANCES': 'Home and Kitchen',\n",
    "'HOME CARE': 'Home and Kitchen',\n",
    "'LADIESWEAR': 'Clothing',\n",
    "'LAWN AND GARDEN': 'Others',\n",
    "'LINGERIE': 'Clothing',\n",
    "'LIQUOR,WINE,BEER': 'Beverages',\n",
    "'MAGAZINES': 'Others',\n",
    "'MEATS': 'Food',\n",
    "'PERSONAL CARE': 'Personal Care',\n",
    "'PET SUPPLIES': 'Others',\n",
    "'PLAYERS AND ELECTRONICS': 'Others',\n",
    "'POULTRY': 'Food',\n",
    "'PREPARED FOODS': 'Food',\n",
    "'PRODUCE': 'Food',\n",
    "'SCHOOL AND OFFICE SUPPLIES': 'Others',\n",
    "'SEAFOOD': 'Food'\n",
    "})\n",
    "\n",
    "data['Holiday_type'] = np.where(data['Holiday_type'].isin(['Holiday', \n",
    "                                                                     'Additional', 'Event', 'Transfer', 'Bridge']), \n",
    "                                                                                                                'Holiday', 'Workday')\n",
    "data = data.drop(['locale', 'locale_name', 'description', 'state', 'transferred'], axis=1)\n",
    "\n",
    "\n",
    "# Identify numeric and non-numeric columns\n",
    "num_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = data.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "# Creating imputer variables\n",
    "numerical_imputer = SimpleImputer(strategy = \"mean\")\n",
    "categorical_imputer = SimpleImputer(strategy = \"most_frequent\")\n",
    "\n",
    "\n",
    "# Define the column transformer\n",
    "categorical_features = cat_cols\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', categories='auto', sparse=False))\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# resample numeric columns by mean and categorical columns by mode\n",
    "resampled = data.resample('D').agg({**{col: 'mean' for col in num_cols}, **{col: (lambda x: x.mode()[0] if not x.mode().empty else np.nan) for col in cat_cols}}).reset_index()\n",
    "\n",
    "resampled = resampled.drop('date', axis=1)\n",
    "\n",
    "\n",
    "# Filling missing values in numerical features of training set\n",
    "resampled[num_cols] = numerical_imputer.fit_transform(resampled[num_cols])\n",
    "\n",
    "resampled[cat_cols] = categorical_imputer.fit_transform(resampled[cat_cols])\n",
    "\n",
    "# Filling missing values in numerical features of evaluation set\n",
    "#X_eval_df[num_cols] = numerical_imputer.transform(X_eval_df[num_cols])\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# Calculate the number of rows in the data\n",
    "\n",
    "n_rows = resampled.shape[0]\n",
    "\n",
    "# Calculate the split point\n",
    "split_point = int(n_rows * 0.90)\n",
    "\n",
    "# Select the first 85% of the rows as the training data\n",
    "X_train = resampled.iloc[:split_point]\n",
    "y_train = X_train['sales']\n",
    "X_train = X_train.drop('sales', axis=1)\n",
    "\n",
    "# Select the remaining 15% of the rows as the validation data\n",
    "X_eval = resampled.iloc[split_point:]\n",
    "y_eval = X_eval['sales']\n",
    "X_eval = X_eval.drop('sales', axis=1)\n",
    "\n",
    "\n",
    "'''creating copy of the categorical features and numerical features\n",
    "before imputing null value to avoid modifying the orginal dataset'''\n",
    "\n",
    "num_cols.remove('sales')  # remove 'sales' from num_cols\n",
    "\n",
    "X_train_cat = X_train[cat_cols].copy()\n",
    "X_train_num = X_train[num_cols].copy()\n",
    "\n",
    "\n",
    "X_eval_cat = X_eval[cat_cols].copy()\n",
    "X_eval_num = X_eval[num_cols].copy()\n",
    "\n",
    "\n",
    "\n",
    "# Fitting the Imputer\n",
    "X_train_cat_imputed = categorical_imputer.fit_transform(X_train_cat)\n",
    "X_train_num_imputed = numerical_imputer.fit_transform(X_train_num)\n",
    "\n",
    "X_eval_cat_imputed = categorical_imputer.fit_transform(X_eval_cat)\n",
    "X_eval_num_imputed = numerical_imputer.fit_transform(X_eval_num)\n",
    "\n",
    "\n",
    "encoder=OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# encoding the xtrain categories and converting to a dataframe\n",
    "X_train_cat_encoded = encoder.fit(X_train_cat_imputed)\n",
    "X_train_cat_encoded = pd.DataFrame(encoder.transform(X_train_cat_imputed).toarray(),\n",
    "                                   columns=encoder.get_feature_names_out(cat_cols))\n",
    "\n",
    "# encoding the xeval categories and converting to a dataframe\n",
    "X_eval_cat_encoded = encoder.fit(X_eval_cat_imputed)\n",
    "X_eval_cat_encoded = pd.DataFrame(encoder.transform(X_eval_cat_imputed).toarray(),\n",
    "                                   columns=encoder.get_feature_names_out(cat_cols))\n",
    "\n",
    "\n",
    "scaler= StandardScaler()\n",
    "\n",
    "X_train_num_scaled = scaler.fit_transform(X_train_num_imputed)\n",
    "X_train_num_sc = pd.DataFrame(X_train_num_scaled, columns = num_cols)\n",
    "\n",
    "X_eval_num_scaled = scaler.fit_transform(X_eval_num_imputed)\n",
    "X_eval_num_sc = pd.DataFrame(X_eval_num_scaled, columns = num_cols)\n",
    "\n",
    "X_train_df = pd.concat([X_train_num_sc,X_train_cat_encoded], axis =1)\n",
    "X_eval_df = pd.concat([X_eval_num_sc,X_eval_cat_encoded], axis =1)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# create a dictionary of models to fit\n",
    "models = {\n",
    "    'Random Forest Regressor': RandomForestRegressor(),\n",
    "    'Decision Tree Regressor': DecisionTreeRegressor(),\n",
    "    'Gradient Boosting Regressor': GradientBoostingRegressor()\n",
    "}\n",
    "\n",
    "# iterate over the models and fit each one to the training data\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_df, y_train)\n",
    "    \n",
    "# evaluate each model using cross-validation\n",
    "rmsle_scores = {}\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train_df, y_train, cv=50, scoring='neg_mean_squared_log_error')\n",
    "    rmsle_scores[name] = np.sqrt(-scores.mean())\n",
    "    \n",
    "# print the RMSLE scores for each model\n",
    "for name, score in rmsle_scores.items():\n",
    "    print(f'{name}: {score}')\n",
    "\n",
    "# choose the model with the lowest RMSLE score\n",
    "best_model_name = min(rmsle_scores, key=rmsle_scores.get)\n",
    "best_model = models[best_model_name]\n",
    "print(f'Best model: {best_model_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4faca2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "import os\n",
    "\n",
    "# set the destination path to the \"export\" directory\n",
    "destination = \".\"\n",
    "\n",
    "# create a dictionary to store the objects and their filenames\n",
    "models = {\"numerical_imputer\": numerical_imputer,\n",
    "          \"categorical_imputer\": categorical_imputer,\n",
    "          \"encoder\": encoder,\n",
    "          \"scaler\": scaler,\n",
    "          \"Final_model\": best_model}\n",
    "\n",
    "# loop through the models and save them using joblib.dump()\n",
    "for name, model in models.items():\n",
    "    dump(model, os.path.join(destination, f\"{name}.joblib\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214a16c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61921d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46abe669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pipreqs\n",
      "  Downloading pipreqs-0.4.11-py2.py3-none-any.whl (32 kB)\n",
      "Collecting yarg\n",
      "  Downloading yarg-0.1.9-py2.py3-none-any.whl (19 kB)\n",
      "Collecting docopt\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from yarg->pipreqs) (2.25.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->yarg->pipreqs) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->yarg->pipreqs) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->yarg->pipreqs) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->yarg->pipreqs) (2020.12.5)\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=2c0a74905833ab0ed30f37b295e1c7899d2044ee4c448781f41545c64172c56b\n",
      "  Stored in directory: c:\\users\\asiak\\appdata\\local\\pip\\cache\\wheels\\56\\ea\\58\\ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
      "Successfully built docopt\n",
      "Installing collected packages: yarg, docopt, pipreqs\n",
      "Successfully installed docopt-0.6.2 pipreqs-0.4.11 yarg-0.1.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pipreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5712ae0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Successfully saved requirements file in .\\requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!pipreqs . --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1ac0de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['store_nbr',\n",
       " 'sales',\n",
       " 'onpromotion',\n",
       " 'cluster',\n",
       " 'transactions',\n",
       " 'dcoilwtico',\n",
       " 'year',\n",
       " 'month',\n",
       " 'day',\n",
       " 'family',\n",
       " 'city',\n",
       " 'Store_type',\n",
       " 'Holiday_type']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1527029b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1519 entries, 0 to 1518\n",
      "Data columns (total 12 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   store_nbr     1519 non-null   float64\n",
      " 1   onpromotion   1519 non-null   float64\n",
      " 2   cluster       1519 non-null   float64\n",
      " 3   transactions  1519 non-null   float64\n",
      " 4   dcoilwtico    1519 non-null   float64\n",
      " 5   year          1519 non-null   float64\n",
      " 6   month         1519 non-null   float64\n",
      " 7   day           1519 non-null   float64\n",
      " 8   family        1519 non-null   object \n",
      " 9   city          1519 non-null   object \n",
      " 10  Store_type    1519 non-null   object \n",
      " 11  Holiday_type  1519 non-null   object \n",
      "dtypes: float64(8), object(4)\n",
      "memory usage: 142.5+ KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4635322a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Holiday', 'Workday'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled.Holiday_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "612be8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('resampledCmplete.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24df645a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69d1016d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\asiak\\appdata\\roaming\\python\\python38\\site-packages (1.0.2)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.2-cp38-cp38-win_amd64.whl (8.3 MB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.20.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.6.2)\n",
      "Installing collected packages: joblib, scikit-learn\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.0.1\n",
      "    Uninstalling joblib-1.0.1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\joblib-1.0.1.dist-info\\\\direct_url.json'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1784967f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
